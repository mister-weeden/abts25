# AbTS25 Dataset

**Current Dataset Version: `0.1.4`**
Most recent change (`0.1.4` -> `0.1.5`) was updating a dependency only.

**Project Page: [https://scottweeden.online/machine-learning/abts25-segmentation/](https://scottweeden.online/machine-learning/abts25-segmentation/)**

## Timeline
- **April 14**: Training dataset release
- **July 14**: Deadline for short paper
- **July 21 - July 28**: Prediction submissions accepted
- **July 31**: Results announced
- **October 8 or 12**: Satellite event at MICCAI 2025

## News
Check here for the latest news about the AbTS25 dataset and starter code!

## Usage
This repository is meant to serve two purposes:
1. To help you **download the dataset**
2. To allow you to benchmark your model using the **official implementation of the metrics**

We recommend starting by installing this `abts25` package using pip. Once you've done this, you'll be able to use the command-line download entrypoint and call the metrics functions from your own codebase.

### Installation
This should be as simple as cloning and installing with pip.
```bash
git clone https://github.com/mister-weeden/abts25
cd abts25
pip3 install -e .
```

We're running Python 3.10.6 on Ubuntu and we suggest that you do too. If you're running a different version of Python 3 and you discover a problem, please [submit an issue](https://github.com/mister-weeden/abts25/issues/new) and we'll try to help. Python 2 or earlier is not supported. If you're running Windows or MacOS, we will do our best to help but we have limited ability to support these environments.

### Data Download
Once the `abts25` package is installed, you should be able to run the following command from the terminal.
```bash
abts25_download_data
```
This will place the data in the `dataset/` folder.

### Using the Metrics
We provide a reference implementation for the metrics and the ranking scheme so that participants know exactly how their algorithm is going to be validated. We strongly encourage using these implementations for model selection during development. In order to do this, we recommend training all your models as cross-validations, so that you have predictions for the entire training set with which you can meaningfully evaluate your approaches.

#### Compute metrics for your predictions
After installing the AbTS25 repository, the following console command is available to you: `abts25_compute_metrics`. You can use it to evaluate predictions against the training ground truth. It's as simple as
```bash
abts25_compute_metrics FOLDER_WITH_PREDICTIONS -num_processes XX
```
This will produce a `evaluation.csv` in FOLDER_WITH_PREDICTIONS with the computed Dice and surface Dice scores.

#### Ranking code
Ranking is based on first averaging your dice and surface dice scores across all cases and HECs, resulting in two values: your average Dice and average Surface Dice. We then use 'rank-then-aggregate' to merge these metrics into a final ranking.

Here are the steps you need to do to run the ranking locally (for example, to find the best configuration for your submission):
1) Execute `generate_summary_csv`, located in [ranking.py](ranking.py). The documentation will tell you how
2) Then use `rank_participants` from the same file with the summary.csv generated by `generate_summary_csv` to generate the final ranking.