Method

Problem Definition
Traditional segmentation approaches follow a task-specific paradigm, where each model is trained for a specific segmentation task. Given a dataset containing image-label pairs, the model learns a direct mapping from the image space to the segmentation mask space, such that for an image, the predicted segmentation mask is given by the model output.

In contrast, we formulate an in-context medical image segmentation framework. Given a support set containing reference image-label pairs and a query image, a single model predicts the segmentation mask for the query image conditioned on the support set. For multi-class segmentation tasks, we decompose the problem into multiple binary segmentation tasks.

Iris Architecture
Iris introduces a novel in-context learning architecture that decouples task encoding from segmentation inference. This design comprises two key components: (1) a task encoding module that distills task-specific information from reference examples into compact task embeddings, and (2) a mask decoding module that leverages these task embeddings to guide query image segmentation.

Task Encoding Module
Given a reference 3D image-label pair, our task encoding module extracts task representations through two parallel streams to extract comprehensive task representations.

Foreground feature encoding. Medical data volumes present unique challenges in feature extraction due to the presence of fine boundary details and anatomical structures spanning only a tiny portion of voxels. Direct feature pooling at downsampled resolution can lead to information loss or complete disappearance of these critical regions of interest (ROIs). To address this hurdle, we opt in a high-resolution foreground feature encoding process. Given features extracted by the encoder, where dimensions are downsampled with ratio r, we compute the foreground embedding by pooling the upsampled features masked by the original high-resolution mask. By applying the original high-resolution mask directly to the upsampled features, we ensure a precise capture of fine anatomical details and small structures that are vital for medical object segmentation.

Contextual feature encoding. The above encoding process extracted foreground features, but lacks important global context information. We encode these contextual information using learnable query tokens. To efficiently process high-resolution features while managing memory constraints, we employ strategy similar to sub-pixel convolution. For feature map, we first expand spatial dimensions while reducing channels using PixelShuffle. After concatenating with the binary mask, we apply a 1×1×1 convolution and PixelUnshuffle to return to the original feature resolution. This approach permits a memory-efficient, high-resolution, feature-mask fusion. The merged features then interact with learnable query tokens through cross-attention and self-attention layers to produce contextual embedding. The final task embedding combines both aspects.

For multi-class segmentation, we generate separate task embeddings for each category. This setting maintains a strong efficiency as the computationally intensive feature extraction is shared across classes while the task encoding module remains lightweight.

Mask Decoding Module
The decoder employs a query-based architecture that efficiently handles both single and multi-class segmentation tasks. For a query image with features, the task encoding module generates class-specific embeddings for each class defined in reference image-label pairs. These embeddings are concatenated into a combined task representation, where K is the number of target classes and K=1 for single-class segmentation. The bidirectional cross-attention mechanism processes this representation, where the updated features enable effective information exchange between class-specific task guidance and query image features. The final segmentation mask is predicted in a single forward pass.

Training
We train Iris in an end-to-end manner using episodic training to simulate in-context learning scenarios. Each training episode consists of sampling reference-query pairs from the same dataset, computing task embeddings from the reference pair, and final predicting segmentation for the query image. The model is optimized using a combination of Dice and cross-entropy losses. To enhance generalization, we employ data augmentation on both query and reference images, add random perturbation to query images to simulate imperfect references, and randomly drop classes in multi-class datasets to encourage independent class-wise task encoding.

Flexible Inference Strategies
After training, Iris supports multiple inference strategies suitable for different practical scenarios.

Efficient one-shot inference. With just one reference example, Iris first encodes the task into compact embeddings that can be stored and reused across multiple query images. Unlike major in-context learning methods to recompute contextual information for each query image, our design greatly eliminates redundant computation. Moreover, Iris can segment multiple classes in a single forward pass, contrasting with methods that require separate passes per class. The minimal storage requirement of these embeddings makes Iris particularly desirable for large-scale data processing pipelines.

Context ensemble. For tasks with multiple reference examples, Iris supports context ensemble for improving performance. We compute task embeddings for each example and average them to create a more robust task representation. This simple averaging strategy combines information from multiple references while maintaining computational efficiency. We extent context ensemble for classes seen during training. Specifically, we maintain a class-specific memory bank that continuously updates task embeddings through exponential moving average (EMA) during the training process. This memory bank stores representative task embeddings for each seen class, enabling direct segmentation for seen classes during inference without requiring context encoding.

Object-level context retrieval. For multi-class segmentation with a pool of reference examples, conventional approaches typically employ image-level retrieval using global embeddings to select semantically similar references. However, this strategy is suboptimal for medical images where multiple anatomical structures coexist, as global embeddings average features across all structures. To enable more precise reference selection, we propose an object-level (class-level) context retrieval strategy. Our approach first encodes class-specific task embeddings for each reference example through our task encoding module - for a reference image with n anatomical classes, we encode n separate task embeddings. For a query image, we obtain initial object segmentation masks using task embeddings from a randomly selected reference. These initial masks are then used to encode n class-specific query task embeddings, which are compared with corresponding reference embeddings in the pool using cosine similarity to select the most similar reference for each class independently. This fine-grained matching allows different structures within the same query image to find their most appropriate references, leading to more accurate segmentation compared to image-level approaches.

In-context tuning. For scenarios requiring adaptation without a full model fine-tuning, Iris offers a lightweight tuning strategy by optimizing only the task embeddings while keeping the model parameters fixed. This tuning process minimizes the segmentation loss between model predictions and ground truth by updating the task embeddings through the gradient descent. In particular, the optimized embeddings can then be stored and reused for similar cases, offering a practical balance between adaptation capability and computational efficiency.
